# Can AI accelerate virtual challenges?


I’ve recently become really enamored with the conceptual framework of “frictionless reproducibility.” You can read more about this [here](https://hdsr.mitpress.mit.edu/pub/g9mau4m0/release/2) or [here](https://hdsr.mitpress.mit.edu/pub/8dqgwqiu/release/1), but the basic idea is that the incredible rate of progress we’ve seen in the data sciences (e.g. empirical machine learning research) is attributable to three “pillars” that have enabled the frictionless reproduction of research results. To quote Ben Recht’s summarization of these pillars or principles: “(1) Researchers should make data easily available and shareable. (2) Researchers should provide easily re-executable code that processes this data to desired ends. (3) Researchers should emphasize competitive testing as a means of evaluation.“ 


The third pillar, competitive testing, has been the one I’ve been thinking the most about. According to Donoho, a competitive test, sometimes itself referred to as a “benchmark” or an “eval” consists of the following components:
```
(1) a shared public dataset, 
(2) a prescribed and quantified task performance metric, 
(3) a set of enrolled competitors seeking to outperform each other on the task, 
(4) and a public leaderboard. 
```
Importantly, competitive testing “can also include **virtual challenges** lacking the formal leaderboard, in which authors still attempt to publish a new state-of-the-art result going beyond previously recorded/published performance levels on a given dataset/performance metric.”

I’ve specifically been thinking about this in the context of a paper I’ve been reviewing for a journal this week. I was initially having a pretty bad time writing my review, as I felt like the paper was remarkably bad, and I was questioning the value of my time in reviewing it. Without de-anonymizing the paper, the fatal flaw in the work is that the authors try to answer the question “is model architecture A better than model architecture B for medical task X,” but train versions of both model architectures that are _significantly_ substandard, making any comparison between their two substandard implementations pretty much worthless. 

This problem (substandard baseline implementations) is pretty pervasive, and can sometimes happen in fairly subtle ways. For example, as [Melis et al. pointed out back in 2018](https://openreview.net/forum?id=ByJHuTgA-), sometimes authors implement substandard baselines by trying a lot more hyperparameters for their proposed new model than for their baseline comparison. More recently, [Jeong et al. demonstrated a similar pitfall](https://arxiv.org/abs/2411.08870v1) when testing “medically-adapted” LLMs and VLMs, where new models were tested with significantly more optimized prompts than their “standard” baselines comparators were given. Efforts to correct these pitfalls in LLM evaluations are already underway, as libraries like the [EleutherAI lm-eval harness can help standardize prompts across models tested](https://arxiv.org/html/2405.14782v2). This library has been implemented as the backend of the popular [HuggingFace OpenLLM leaderboard](https://huggingface.co/open-llm-leaderboard). 

But back to the particular paper I was reviewing -- how did it end up implementing such substandard models? (As an initial step in part of a much larger pipeline that I’ll omit here out of respect for the authors’ anonymity in peer review), the authors looked at the binary classification accuracy for the task of differentiating Normal vs Abnormal images in the [VinDR-CXR dataset](https://vindr.ai/datasets/cxr). 

What is the state-of-the-art model for that metric on this dataset? It’s actually a bit harder to find than you might think. Despite the fact that VinDR-CXR is a freely-available public dataset, and has even been part of a [large Kaggle challenge with a public leaderboard](https://www.kaggle.com/c/vinbigdata-chest-xray-abnormalities-detection/), the leaderboard for that challenge focused on a different metric than the authors of this paper were interested in. It focused on an object detection metric for multiple potential pathologies within an image. 

So is the incommensurability of these metrics the fault of the authors? Should they have just measured their systems’ accuracies on the same object detection task from the Kaggle leaderboard to enable a more fair comparison? I don’t think that they necessarily should have. You can imagine that rather than a localizing object detection system, you might be evaluating systems for an application like full-image triage where you care more about global anomaly detection. In this case, the authors’ chosen metric might be more appropriate than the one on the leaderboard.

So without an explicit public leaderboard to easily determine the state of the art, how do you go about determining the optimal baseline for comparison? This corresponds to the _virtual challenge_ state described by Donoho above. For this specific paper I was reviewing, the answer seemed to be that the authors just didn’t put in any effort at all to pick good baselines. 

The better answer is that you actually have to open papers and read them. Querying the name of the dataset in Google Scholar, I had to go through 4 papers, and even skim them pretty closely, before I found one that had data describing a model’s performance in a setting and metric relevant to the one for the paper I was reviewing. By combining the name of the dataset and “accuracy” in my query, I was able to bump that paper to the first result, but I still had to open it up and read it. The results weren’t present in the abstract alone. 

I also experimented with using FutureHouse’s [“hasanyone.com” application](hasanyone.com) to try to answer a natural language version of the query for me. [It did a very good job of this, found the relevant paper, and actually pulled out the relevant sensitivity and specificity I cared about too](https://hasanyone.com/?id=effda7c4#). I sort of wish it had synthesized the sens/spec directly into accuracy, which should have been possible because the paper actually contained info on the prevalence of “any finding” within the test set (Accuracy = (Sensitivity * Prevalence) + (Specificity * (1 - Prevalence)), or ~90% in this case). But overall, it answered my question pretty well. 

The success of “hasanyone” at answering my query made me excited about the idea of using LLM agents to take “virtual competitions,” and provide them with more explicit and automatically generated leaderboards. For a given (1) public dataset and (2) task performance metric, can a system search through all of the literature for other “competitors” who also worked on (1) and (2) and build an automatically generated public leaderboard of their performance on this task? If so, would that accelerate the process of establishing good baselines, and make reproducibility in that area smoother? Would this have saved the authors of this paper the effort of writing up results that ultimately won’t have any impact on the problem they care about?

I can easily imagine some objections here, namely that perhaps it isn’t a bad thing for people to open up papers and actually read (or even skim) them. But my counterpoint would be that people clearly aren’t doing that right now, so you can’t decrease reading levels below 0, and consequently this should exclusively have a net positive effect on how researchers are expending their efforts. 

Building a system like this wouldn’t be trivial, for a number of reasons. For example, the searches shown here weren’t exhaustive. This was fine while I was reviewing this paper, because it sufficed for my purposes to find the existence of a single model with significantly better performance than the authors had described. But for a leaderboard you’d want a more exhaustive search to find ALL papers that compete at your dataset/metric of interest.  I do think someone could build a very good system on the backbone of a program like [PaperQA](https://github.com/Future-House/paper-qa) or [OpenScholar](https://openscholar.allen.ai) or whatever is going on in the backend of [Undermind](https://www.undermind.ai/home/). 

It’s sort of meta and dumb, but funnily enough I think you could take the task I’m describing here (automatically building concrete leaderboards for virtual competitions) and actually build a really good competitive test for scientific agent systems out of this task. There are obviously pretty tricky things involved in trying to crawl the breadth of the existing literature, identify when published eval setups are sufficiently similar to a query of interest, calculate some metrics of interest out of other numbers presented in the text of a paper, and maybe even ideally diving into supplemental data to process continuous metrics like AUROC into possible accuracies across different binary thresholds.

So why am I blogging about this rather than building it? Under normal circumstances, I’d just start pursuing this idea myself, because I think it sounds exciting and fun and potentially useful. However, I’m in the midst of a particularly grueling stretch of hospital shifts for the next 6 weeks, and ultimately am sort of out of energy for being ``protective'' over research ideas. So I thought it would be most fun to just write a post about it for now. If you have thoughts, interest, ideas about this — if you want to steal it wholesale — if you think it’s dumb and there are ways im thinking about it badly — please reach out to me on Twitter or Bluesky or email or something! Mostly I think I'm just really excited by the framework of frictionless reproducibility for understanding research progress in both the existing data sciences (like empirical ML), and starting to think about how that framework can be extended to significantly ``less smooth'' spaces like biotech/medicine/etc, and would love to chat more with other people who are thinking about the same things.
